import streamlit as st
import uuid
from rag_utils import initial_retrieval, rerank, build_prompt, ask_llm


INITIAL_K = 10
RERANK_TOP_K = 5

st.set_page_config(page_title="Legal RAG Chat", layout="wide")
st.title("Legal Chatbot Supporter")

if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "selected_chat_index" not in st.session_state:
    st.session_state.selected_chat_index = None
# st.session_state.setdefault("chat_history", [])
# st.session_state.setdefault("retrieved_chunks", [])

with st.sidebar:
    st.markdown("### L·ªãch s·ª≠ h·ªôi tho·∫°i")
    for idx, chat in enumerate(st.session_state.chat_history):
        if st.button(f"üí¨ {chat['answer'][:50]}...", key=f"select_{idx}"):
            st.session_state.selected_chat_index = idx
    st.markdown("---")
    if st.button("Clear conversation"):
        st.session_state.chat_history = []
        st.session_state.selected_chat_index = None
        st.success("Cleared.")

# Input box
query = st.chat_input("Nh·∫≠p c√¢u h·ªèi:")

if query:
    if st.session_state.selected_chat_index is not None:
        # L·∫•y l·∫°i chunks t·ª´ c√¢u tr·∫£ l·ªùi ƒë√£ ch·ªçn
        prev_chat = st.session_state.chat_history[st.session_state.selected_chat_index]
        top_chunks = prev_chat["chunks"]
        context_info = "Ti·∫øp t·ª•c t·ª´ c√¢u tr·∫£ l·ªùi ƒë√£ ch·ªçn."
        st.session_state.selected_chat_index = None
    else:
        # Retrieval + rerank m·ªõi
        with st.spinner("Retrieving relevant chunks..."):
            initial = initial_retrieval(query, k=INITIAL_K)
            top_chunks = rerank(query, initial, top_k=RERANK_TOP_K)
        context_info = "C√¢u h·ªèi m·ªõi."

    # Build prompt and call LLM
    prompt = build_prompt(query, top_chunks)
    messages = [
        {"role": "system", "content": "B·∫°n l√† m·ªôt ng∆∞·ªùi tr·ª£ l√Ω chuy√™n t√¨m ki·∫øm v√† tr·∫£ l·ªùi v·ªÅ th√¥ng tin vƒÉn b·∫£n h√†nh ch√≠nh."},
    ]
    # Th√™m l·ªãch s·ª≠ h·ªôi tho·∫°i tr∆∞·ªõc (ƒë√£ c√≥ user+assistant)
    for chat in st.session_state.chat_history:
        messages.append({"role": "user", "content": chat["question"]})
        messages.append({"role": "assistant", "content": chat["answer"]})

    messages.append({"role": "user", "content": prompt})
    with st.spinner("Generating answer from OpenAI..."):
        answer = ask_llm(messages)

    st.subheader("Answer")
    st.session_state.chat_history.append({
        "question": query,
        "answer": answer,
        "chunks": top_chunks
    })

    # Hi·ªÉn th·ªã
    with st.chat_message("user"):
        st.markdown(query)
    with st.chat_message("assistant"):
        st.markdown(answer)

    # Hi·ªÉn th·ªã ngu·ªìn
    st.subheader("Sources Summary")
    st.write(f"**Ngu·ªìn:** {context_info}")
    for i, c in enumerate(top_chunks, 1):
        meta = c.metadata
        st.markdown(
            f"{i}. **{meta.get('so_ky_hieu','')}** chunk_index={meta.get('chunk_index')} "
            f"(rerank_score={c.score_rerank:.4f}) ‚Äî trich_yeu: {meta.get('trich_yeu','')}"
        )